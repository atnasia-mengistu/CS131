{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46c8ec2-0940-4839-85d3-524bcb7a28c9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Assignment 4: PySpark MLlib - NYC Taxi Fare Prediction\n",
    "\n",
    "**Author:** Prasham Sheth\n",
    "**Course:** CS131  \n",
    "**Date:** 05/04/2025\n",
    "\n",
    "In this assignment, we use PySpark MLlib to predict NYC taxi `total_amount` using a Decision Tree Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f43640-cae8-4786-8519-bce63531bc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 21:37:11 WARN Utils: Your hostname, Prashams-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.6 instead (on interface en0)\n",
      "25/05/05 21:37:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/05 21:37:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"NYC Taxi MLlib\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7ce1b-f5d9-4d14-9b2e-a988d560efd1",
   "metadata": {},
   "source": [
    "## Load the Dataset and Select Required Columns\n",
    "\n",
    "We use only:\n",
    "- `passenger_count` (4th col)\n",
    "- `pulocationid` (8th col)\n",
    "- `dolocationid` (9th col)\n",
    "- `total_amount` (17th col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fb81394-d9dc-4b49-8b07-3294bc29e75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+------------+------------+\n",
      "|passenger_count|pulocationid|dolocationid|total_amount|\n",
      "+---------------+------------+------------+------------+\n",
      "|            1.0|       151.0|       239.0|        9.95|\n",
      "|            1.0|       239.0|       246.0|        16.3|\n",
      "|            3.0|       236.0|       236.0|         5.8|\n",
      "|            5.0|       193.0|       193.0|        7.55|\n",
      "|            5.0|       193.0|       193.0|       55.55|\n",
      "|            5.0|       193.0|       193.0|       13.31|\n",
      "|            5.0|       193.0|       193.0|       55.55|\n",
      "|            1.0|       163.0|       229.0|        9.05|\n",
      "|            1.0|       229.0|         7.0|        18.5|\n",
      "|            2.0|       141.0|       234.0|        13.0|\n",
      "+---------------+------------+------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load CSV\n",
    "df = spark.read.csv(\"2019-01-h1.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Select required columns and rename for clarity\n",
    "selected = df.select(\n",
    "    df.columns[3],   # passenger_count (4th col)\n",
    "    df.columns[7],   # pulocationid (8th col)\n",
    "    df.columns[8],   # dolocationid (9th col)\n",
    "    df.columns[16],  # total_amount (17th col)\n",
    ").toDF(\"passenger_count\", \"pulocationid\", \"dolocationid\", \"total_amount\")\n",
    "\n",
    "# Show first 10 entries\n",
    "selected.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9260588-09a9-4ad0-bbfd-297bd26e0219",
   "metadata": {},
   "source": [
    "## Split the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44407761-cc50-4e03-ba20-c1cbf887469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set count: 2922047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set count: 728952\n",
      "Sample trainDF:\n",
      "+---------------+------------+------------+------------+\n",
      "|passenger_count|pulocationid|dolocationid|total_amount|\n",
      "+---------------+------------+------------+------------+\n",
      "|            0.0|         1.0|         1.0|        90.0|\n",
      "|            0.0|         1.0|         1.0|      116.75|\n",
      "|            0.0|         4.0|         4.0|        5.75|\n",
      "|            0.0|         4.0|        17.0|        20.3|\n",
      "|            0.0|         4.0|        68.0|        15.8|\n",
      "+---------------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Sample testDF:\n",
      "+---------------+------------+------------+------------+\n",
      "|passenger_count|pulocationid|dolocationid|total_amount|\n",
      "+---------------+------------+------------+------------+\n",
      "|            0.0|         4.0|         4.0|         4.3|\n",
      "|            0.0|         4.0|        79.0|         5.8|\n",
      "|            0.0|         4.0|        90.0|        10.8|\n",
      "|            0.0|         4.0|       170.0|        12.7|\n",
      "|            0.0|         7.0|         7.0|         9.1|\n",
      "+---------------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 80% train, 20% test\n",
    "trainDF, testDF = selected.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set count: {trainDF.count()}\")\n",
    "print(f\"Test set count: {testDF.count()}\")\n",
    "\n",
    "print(\"Sample trainDF:\")\n",
    "trainDF.show(5)\n",
    "print(\"Sample testDF:\")\n",
    "testDF.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd2575-9cee-4d38-9129-5d96c15717e2",
   "metadata": {},
   "source": [
    "## Build a Decision Tree Regressor Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60e56fe5-a3ac-4255-a489-d2b40d8df7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Assemble features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"passenger_count\", \"pulocationid\", \"dolocationid\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Decision Tree Regressor\n",
    "dt = DecisionTreeRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"total_amount\"\n",
    ").setMaxBins(1000)\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, dt])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f124c63-6bd7-4dbd-90a3-efff928ffac5",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d36db2-7ebf-4e48-b233-e4784f2a0c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_9 in memory! (computed 28.4 MiB so far)\n",
      "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_1 in memory! (computed 8.0 MiB so far)\n",
      "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_9 to disk instead.\n",
      "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_1 to disk instead.\n",
      "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_8 in memory! (computed 8.0 MiB so far)\n",
      "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_8 to disk instead.\n",
      "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_7 in memory! (computed 8.0 MiB so far)\n",
      "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_0 in memory! (computed 8.0 MiB so far)\n",
      "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_0 to disk instead.\n",
      "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_3 in memory! (computed 8.0 MiB so far)\n",
      "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_3 to disk instead.\n",
      "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_7 to disk instead.\n",
      "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_2 in memory! (computed 12.0 MiB so far)\n",
      "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_2 to disk instead.\n",
      "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_5 in memory! (computed 12.0 MiB so far)\n",
      "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_5 to disk instead.\n",
      "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_4 in memory! (computed 12.0 MiB so far)\n",
      "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_4 to disk instead.\n",
      "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_6 in memory! (computed 18.0 MiB so far)\n",
      "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_6 to disk instead.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline\n",
    "model = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbcba5f-5f10-4913-9d80-2ccefa3de731",
   "metadata": {},
   "source": [
    "## Make Predictions on the Test Set and Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd7cf4f8-cfd8-4090-8274-f8844cde3633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+------------+------------+------------------+\n",
      "|passenger_count|pulocationid|dolocationid|total_amount|        prediction|\n",
      "+---------------+------------+------------+------------+------------------+\n",
      "|            0.0|         4.0|         4.0|         4.3|17.919089195752292|\n",
      "|            0.0|         4.0|        79.0|         5.8|17.919089195752292|\n",
      "|            0.0|         4.0|        90.0|        10.8|17.919089195752292|\n",
      "|            0.0|         4.0|       170.0|        12.7|17.919089195752292|\n",
      "|            0.0|         7.0|         7.0|         9.1|17.919089195752292|\n",
      "|            0.0|         7.0|       138.0|        10.8|17.919089195752292|\n",
      "|            0.0|         7.0|       179.0|         3.8|17.919089195752292|\n",
      "|            0.0|        10.0|       232.0|       60.72|17.919089195752292|\n",
      "|            0.0|        13.0|        45.0|         9.8|17.919089195752292|\n",
      "|            0.0|        13.0|        45.0|       13.55|17.919089195752292|\n",
      "+---------------+------------+------------+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "predictions = model.transform(testDF)\n",
    "\n",
    "# Show predictions with features (first 10 entries)\n",
    "predictions.select(\n",
    "    \"passenger_count\", \"pulocationid\", \"dolocationid\", \"total_amount\", \"prediction\"\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f865cf6f-4a53-458a-bc76-3e86330f9359",
   "metadata": {},
   "source": [
    "## Evaluate the Model with RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8e9009f-bd96-48c9-bb20-3f9a8a0562bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 47.19257906771566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"total_amount\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77528af3-1ce8-48b1-b45e-d13b8a2a97e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1832b1-48f6-4bff-8746-a9f9d95d82a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
