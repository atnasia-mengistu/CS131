{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b46c8ec2-0940-4839-85d3-524bcb7a28c9",
      "metadata": {
        "scrolled": true,
        "id": "b46c8ec2-0940-4839-85d3-524bcb7a28c9"
      },
      "source": [
        "Atnasia Mengistu  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f43640-cae8-4786-8519-bce63531bc50",
      "metadata": {
        "id": "b7f43640-cae8-4786-8519-bce63531bc50"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"NYC Taxi MLlib\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcd7ce1b-f5d9-4d14-9b2e-a988d560efd1",
      "metadata": {
        "id": "dcd7ce1b-f5d9-4d14-9b2e-a988d560efd1"
      },
      "source": [
        "## Load the Dataset and Select Required Columns\n",
        "\n",
        "We use only:\n",
        "- `passenger_count` (4th col)\n",
        "- `pulocationid` (8th col)\n",
        "- `dolocationid` (9th col)\n",
        "- `total_amount` (17th col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3fb81394-d9dc-4b49-8b07-3294bc29e75a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "3fb81394-d9dc-4b49-8b07-3294bc29e75a",
        "outputId": "972fdbdd-00e3-46a3-abae-9a96dc86a03a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/2019-01-h1.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5a2c8c708093>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2019-01-h1.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Select required columns and rename for clarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m selected = df.select(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/2019-01-h1.csv."
          ]
        }
      ],
      "source": [
        "# Load CSV\n",
        "df = spark.read.csv(\"2019-01-h1.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Select required columns and rename for clarity\n",
        "selected = df.select(\n",
        "    df.columns[3],   # passenger_count (4th col)\n",
        "    df.columns[7],   # pulocationid (8th col)\n",
        "    df.columns[8],   # dolocationid (9th col)\n",
        "    df.columns[16],  # total_amount (17th col)\n",
        ").toDF(\"passenger_count\", \"pulocationid\", \"dolocationid\", \"total_amount\")\n",
        "\n",
        "# Show first 10 entries\n",
        "selected.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9260588-09a9-4ad0-bbfd-297bd26e0219",
      "metadata": {
        "id": "c9260588-09a9-4ad0-bbfd-297bd26e0219"
      },
      "source": [
        "## Split the Data into Training and Testing Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44407761-cc50-4e03-ba20-c1cbf887469d",
      "metadata": {
        "id": "44407761-cc50-4e03-ba20-c1cbf887469d",
        "outputId": "7979bc87-e5a3-4ffc-9b4f-b4240e10e3a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set count: 2922047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set count: 728952\n",
            "Sample trainDF:\n",
            "+---------------+------------+------------+------------+\n",
            "|passenger_count|pulocationid|dolocationid|total_amount|\n",
            "+---------------+------------+------------+------------+\n",
            "|            0.0|         1.0|         1.0|        90.0|\n",
            "|            0.0|         1.0|         1.0|      116.75|\n",
            "|            0.0|         4.0|         4.0|        5.75|\n",
            "|            0.0|         4.0|        17.0|        20.3|\n",
            "|            0.0|         4.0|        68.0|        15.8|\n",
            "+---------------+------------+------------+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Sample testDF:\n",
            "+---------------+------------+------------+------------+\n",
            "|passenger_count|pulocationid|dolocationid|total_amount|\n",
            "+---------------+------------+------------+------------+\n",
            "|            0.0|         4.0|         4.0|         4.3|\n",
            "|            0.0|         4.0|        79.0|         5.8|\n",
            "|            0.0|         4.0|        90.0|        10.8|\n",
            "|            0.0|         4.0|       170.0|        12.7|\n",
            "|            0.0|         7.0|         7.0|         9.1|\n",
            "+---------------+------------+------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 80% train, 20% test\n",
        "trainDF, testDF = selected.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(f\"Training set count: {trainDF.count()}\")\n",
        "print(f\"Test set count: {testDF.count()}\")\n",
        "\n",
        "print(\"Sample trainDF:\")\n",
        "trainDF.show(5)\n",
        "print(\"Sample testDF:\")\n",
        "testDF.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43dd2575-9cee-4d38-9129-5d96c15717e2",
      "metadata": {
        "id": "43dd2575-9cee-4d38-9129-5d96c15717e2"
      },
      "source": [
        "## Build a Decision Tree Regressor Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60e56fe5-a3ac-4255-a489-d2b40d8df7e6",
      "metadata": {
        "id": "60e56fe5-a3ac-4255-a489-d2b40d8df7e6"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Assemble features\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"passenger_count\", \"pulocationid\", \"dolocationid\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "# Decision Tree Regressor\n",
        "dt = DecisionTreeRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"total_amount\"\n",
        ").setMaxBins(1000)\n",
        "\n",
        "# Pipeline\n",
        "pipeline = Pipeline(stages=[assembler, dt])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f124c63-6bd7-4dbd-90a3-efff928ffac5",
      "metadata": {
        "id": "3f124c63-6bd7-4dbd-90a3-efff928ffac5"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d36db2-7ebf-4e48-b233-e4784f2a0c65",
      "metadata": {
        "id": "83d36db2-7ebf-4e48-b233-e4784f2a0c65",
        "outputId": "051bf418-ee77-4694-e514-b8e067233357"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_9 in memory! (computed 28.4 MiB so far)\n",
            "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_1 in memory! (computed 8.0 MiB so far)\n",
            "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_9 to disk instead.\n",
            "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_1 to disk instead.\n",
            "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_8 in memory! (computed 8.0 MiB so far)\n",
            "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_8 to disk instead.\n",
            "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_7 in memory! (computed 8.0 MiB so far)\n",
            "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_0 in memory! (computed 8.0 MiB so far)\n",
            "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_0 to disk instead.\n",
            "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_3 in memory! (computed 8.0 MiB so far)\n",
            "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_3 to disk instead.\n",
            "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_7 to disk instead.\n",
            "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_2 in memory! (computed 12.0 MiB so far)\n",
            "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_2 to disk instead.\n",
            "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_5 in memory! (computed 12.0 MiB so far)\n",
            "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_5 to disk instead.\n",
            "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_4 in memory! (computed 12.0 MiB so far)\n",
            "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_4 to disk instead.\n",
            "25/05/05 21:42:57 WARN MemoryStore: Not enough space to cache rdd_51_6 in memory! (computed 18.0 MiB so far)\n",
            "25/05/05 21:42:57 WARN BlockManager: Persisting block rdd_51_6 to disk instead.\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Fit the pipeline\n",
        "model = pipeline.fit(trainDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dbcba5f-5f10-4913-9d80-2ccefa3de731",
      "metadata": {
        "id": "7dbcba5f-5f10-4913-9d80-2ccefa3de731"
      },
      "source": [
        "## Make Predictions on the Test Set and Show Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd7cf4f8-cfd8-4090-8274-f8844cde3633",
      "metadata": {
        "id": "dd7cf4f8-cfd8-4090-8274-f8844cde3633",
        "outputId": "8b1f9ee3-e286-4ee3-eee5-fea5863864ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------+------------+------------+------------+------------------+\n",
            "|passenger_count|pulocationid|dolocationid|total_amount|        prediction|\n",
            "+---------------+------------+------------+------------+------------------+\n",
            "|            0.0|         4.0|         4.0|         4.3|17.919089195752292|\n",
            "|            0.0|         4.0|        79.0|         5.8|17.919089195752292|\n",
            "|            0.0|         4.0|        90.0|        10.8|17.919089195752292|\n",
            "|            0.0|         4.0|       170.0|        12.7|17.919089195752292|\n",
            "|            0.0|         7.0|         7.0|         9.1|17.919089195752292|\n",
            "|            0.0|         7.0|       138.0|        10.8|17.919089195752292|\n",
            "|            0.0|         7.0|       179.0|         3.8|17.919089195752292|\n",
            "|            0.0|        10.0|       232.0|       60.72|17.919089195752292|\n",
            "|            0.0|        13.0|        45.0|         9.8|17.919089195752292|\n",
            "|            0.0|        13.0|        45.0|       13.55|17.919089195752292|\n",
            "+---------------+------------+------------+------------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# Predict\n",
        "predictions = model.transform(testDF)\n",
        "\n",
        "# Show predictions with features (first 10 entries)\n",
        "predictions.select(\n",
        "    \"passenger_count\", \"pulocationid\", \"dolocationid\", \"total_amount\", \"prediction\"\n",
        ").show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f865cf6f-4a53-458a-bc76-3e86330f9359",
      "metadata": {
        "id": "f865cf6f-4a53-458a-bc76-3e86330f9359"
      },
      "source": [
        "## Evaluate the Model with RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8e9009f-bd96-48c9-bb20-3f9a8a0562bd",
      "metadata": {
        "id": "c8e9009f-bd96-48c9-bb20-3f9a8a0562bd",
        "outputId": "7a3c6f8a-2092-4cb2-c232-ec795b354c73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 26:>                                                       (0 + 10) / 10]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error (RMSE) on test data = 47.19257906771566\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=\"total_amount\",\n",
        "    predictionCol=\"prediction\",\n",
        "    metricName=\"rmse\"\n",
        ")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77528af3-1ce8-48b1-b45e-d13b8a2a97e3",
      "metadata": {
        "id": "77528af3-1ce8-48b1-b45e-d13b8a2a97e3"
      },
      "outputs": [],
      "source": [
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da1832b1-48f6-4bff-8746-a9f9d95d82a7",
      "metadata": {
        "id": "da1832b1-48f6-4bff-8746-a9f9d95d82a7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}